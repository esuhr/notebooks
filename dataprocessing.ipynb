{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: *Data Processing*\n",
    "The goal for this notebook is to prepare Citi Bike data (123 files, 36.79GB ) for analysis.\n",
    " \n",
    "\n",
    "From **June 2013 - January 2021**, Citi Bike data is stored in the following format:\n",
    "\n",
    "```\n",
    "'tripduration','starttime','stoptime','start station id','start station name','start station latitude','start station longitude','end station id','end station name','end station latitude','end station longitude','bikeid','usertype','birth year','gender'\n",
    "```\n",
    " \n",
    "\n",
    " \n",
    "From **February 2021 - July 2023**, Citi Bike data is stored in the following format:\n",
    "\n",
    "```\n",
    "ride_id,rideable_type,started_at,ended_at,start_station_name,start_station_id,end_station_name,end_station_id,start_lat,start_lng,end_lat,end_lng,member_casual\n",
    "```\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "After looking at the data, we can notice several differences in how the data is stored:\n",
    "1. Post February 2021, gender, Bike ID, birth year, and trip duration are not stored. These columns can be dropped.\n",
    "2. Station ID in pre-February 2021 data is a four-digit integer (3276 for Marin Light Rail). Post-February 2021, the station ID is changed to a combination of characters and integers (JC008 for Marin Light Rail)\n",
    "3. User Type in pre-February 2021 data is collected as 'Subscriber' or 'Customer'. Post-February 2021, the user type is collected as 'member' or 'casual'. We can convert 'Subscriber' to 'member' and 'Customer' to 'casual' for consistency.\n",
    "4. The column names are different, these can be renamed to match the post-February 2021 data.\n",
    "    - we will need to check if the coordinates for each station are consistent across the two date ranges\n",
    "5. The start and end times are also collected differently. We will need to convert the pre-February 2021 data to match the post-February 2021 data.\n",
    " \n",
    " While Pandas is a great tool for most data processing tasks, it is limited when it comes to processing large datasets. We'll use Polars to process, clean, merge, and format the data. We'll then save the data by year to a parquet file for analysis in the next notebook. A single parquet file for each year will be much easier to manage than 123 csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "First, we'll need to correct for irregular date formatting from 2014-09 to 2021-01.\n",
    "    1/1/2016 00:04:43 -> 2016-01-01 00:04:43\n",
    "    2016-01-01 00:04:43.0000 -> 2016-01-01 00:04:43\n",
    "We'll do this using pandas and rewrite the csv file.\n",
    "'''\n",
    "\n",
    "correction_start = pd.to_datetime('201409', format='%Y%m')\n",
    "correction_end = pd.to_datetime('202101', format='%Y%m')\n",
    "# create date range\n",
    "date_range = pd.date_range(start=correction_start, end=correction_end, freq='M')\n",
    "date_range = [date.strftime('%Y%m') for date in pd.date_range(correction_start, correction_end, freq='MS')]\n",
    "\n",
    "# Read in the data\n",
    "\n",
    "for date in date_range:\n",
    "    df = pd.read_csv(f'./source-data/{date}-citibike-tripdata.csv')\n",
    "    df['starttime'] = pd.to_datetime(df['starttime']).dt.round(freq='S')\n",
    "    df['stoptime'] = pd.to_datetime(df['stoptime']).dt.round(freq='S')\n",
    "    df.to_csv(f'./source-data/{date}-citibike-tripdata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a date range list for each year\n",
    "    - first date range is from 201306 to 202101\n",
    "    - second date range is from 202102 to 202307\n",
    "lists will be used to read in csv files, and concatenate them into a parquet file for each year\n",
    "'''\n",
    "date_ranges_1 = []\n",
    "\n",
    "# first: date strings for 2013-06 to 2013-12\n",
    "start_date = pd.to_datetime('201306', format='%Y%m')\n",
    "end_date = pd.to_datetime('201312', format='%Y%m')\n",
    "date_ranges_1.append([date.strftime('%Y%m') for date in pd.date_range(start_date, end_date, freq='MS')])\n",
    "\n",
    "for year in range(2014, 2021):\n",
    "    start_date = pd.to_datetime(str(year) + '01', format='%Y%m')\n",
    "    end_date = pd.to_datetime(str(year) + '12', format='%Y%m')\n",
    "    date_ranges_1.append([date.strftime('%Y%m') for date in pd.date_range(start_date, end_date, freq='MS')])\n",
    "\n",
    "start_date = pd.to_datetime('202101', format='%Y%m')\n",
    "end_date = pd.to_datetime('202101', format='%Y%m')\n",
    "date_ranges_1.append([date.strftime('%Y%m') for date in pd.date_range(start_date, end_date, freq='MS')])\n",
    "\n",
    "# second: date strings for Feb 2021 - Jul 2023\n",
    "date_ranges_2 = []\n",
    "start_date = pd.to_datetime('20210201', format='%Y%m%d')\n",
    "end_date = pd.to_datetime('20211201', format='%Y%m%d')\n",
    "date_ranges_2.append([date.strftime('%Y%m') for date in pd.date_range(start_date, end_date, freq='MS')])\n",
    "\n",
    "start_date = pd.to_datetime('20220101', format='%Y%m%d')\n",
    "end_date = pd.to_datetime('20221201', format='%Y%m%d')\n",
    "date_ranges_2.append([date.strftime('%Y%m') for date in pd.date_range(start_date, end_date, freq='MS')])\n",
    "\n",
    "start_date = pd.to_datetime('20230101', format='%Y%m%d')\n",
    "end_date = pd.to_datetime('20230701', format='%Y%m%d')\n",
    "date_ranges_2.append([date.strftime('%Y%m') for date in pd.date_range(start_date, end_date, freq='MS')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create a function to loop through date ranges to read each csv file and concat into one polars dataframe\n",
    "'''\n",
    "\n",
    "datapath = './source-data/{}-citibike-tripdata.csv'\n",
    "\n",
    "def create_df(date_range, path):\n",
    "    for date in date_range:\n",
    "        df = pl.read_csv(path.format(date), infer_schema_length=0)\n",
    "        if date == date_range[0]:\n",
    "            df_all = df\n",
    "        else:\n",
    "            df_all = pl.concat([df_all, df])\n",
    "    return df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define columns to be dropped\n",
    "- for this analysis, we'll exclude birth year and gender columns\n",
    "'''\n",
    "columns = ['birth year', 'gender', 'tripduration']\n",
    "\n",
    "'''\n",
    "Function to rename columns\n",
    "- change the column names to maintain consistency across datasets. \n",
    "- data format changes Feb 2021, pre Feb 2021 data is changed to match post Feb 2021 data\n",
    "'''\n",
    "def rename_cols(df):\n",
    "    renames = {\n",
    "            'starttime': 'started_at',\n",
    "            'stoptime': 'ended_at',\n",
    "            'start station id': 'start_station_id',\n",
    "            'start station name': 'start_station_name',\n",
    "            'start station latitude': 'start_lat',\n",
    "            'start station longitude': 'start_lng',\n",
    "            'end station id': 'end_station_id',\n",
    "            'end station name': 'end_station_name',\n",
    "            'end station latitude': 'end_lat',\n",
    "            'end station longitude': 'end_lng',\n",
    "            'bikeid': 'bike_id',\n",
    "            'usertype': 'member_casual',\n",
    "        }\n",
    "    df = df.rename(renames)\n",
    "    return df\n",
    "\n",
    "'''\n",
    "Function to cast types for each column\n",
    "'''\n",
    "def cast_types(df):\n",
    "    out = df.select(\n",
    "        pl.col('started_at').str.strptime(pl.Datetime, '%Y-%m-%d %H:%M:%S', strict=False),\n",
    "        pl.col('ended_at').str.strptime(pl.Datetime, '%Y-%m-%d %H:%M:%S', strict=False),\n",
    "        pl.col('start_station_id').cast(pl.Int32, strict=False),\n",
    "        pl.col('start_station_name').cast(pl.Utf8, strict=False),\n",
    "        pl.col('start_lat').cast(pl.Float32, strict=False),\n",
    "        pl.col('start_lng').cast(pl.Float32, strict=False),\n",
    "        pl.col('end_station_id').cast(pl.Int32, strict=False),\n",
    "        pl.col('end_station_name').cast(pl.Utf8, strict=False),\n",
    "        pl.col('end_lat').cast(pl.Float32, strict=False),\n",
    "        pl.col('end_lng').cast(pl.Float32, strict=False),\n",
    "        pl.col('bike_id').cast(pl.Int32, strict=False),\n",
    "        pl.col('member_casual').cast(pl.Utf8, strict=False),\n",
    "    )\n",
    "    return out\n",
    "\n",
    "'''\n",
    "Function to change values for member_casual column\n",
    "- change 'Subscriber' to 'member' and 'Customer' to 'casual'\n",
    "'''\n",
    "def change_member_casual(df):\n",
    "    map_dict = {\n",
    "        'Subscriber': 'member',\n",
    "        'Customer': 'casual',\n",
    "        None: 'unknown'\n",
    "    }\n",
    "\n",
    "    df = df.with_columns(\n",
    "        pl.col('member_casual').map_dict(map_dict, default='unknown').alias('member_casual')\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date_range in date_ranges_1:\n",
    "    filename = date_range[0][:4] + '.parquet'\n",
    "    df = create_df(date_range, datapath)\n",
    "    df.drop(columns)\n",
    "    df = rename_cols(df)\n",
    "    df = cast_types(df)\n",
    "    df = change_member_casual(df)\n",
    "\n",
    "    df.write_parquet(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second set of date ranges, we'll only need to cast types for each column. We'll copy and modify the cast_types function to handle this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to cast types for each column\n",
    "'''\n",
    "def cast_types_2(df):\n",
    "    out = df.select(\n",
    "        pl.col('ride_id').cast(pl.Utf8, strict=False),\n",
    "        pl.col('rideable_type').cast(pl.Utf8, strict=False),\n",
    "        pl.col('started_at').str.strptime(pl.Datetime, '%Y-%m-%d %H:%M:%S', strict=False),\n",
    "        pl.col('ended_at').str.strptime(pl.Datetime, '%Y-%m-%d %H:%M:%S', strict=False),\n",
    "        pl.col('start_station_id').cast(pl.Int32, strict=False),\n",
    "        pl.col('start_station_name').cast(pl.Utf8, strict=False),\n",
    "        pl.col('start_lat').cast(pl.Float32, strict=False),\n",
    "        pl.col('start_lng').cast(pl.Float32, strict=False),\n",
    "        pl.col('end_station_id').cast(pl.Int32, strict=False),\n",
    "        pl.col('end_station_name').cast(pl.Utf8, strict=False),\n",
    "        pl.col('end_lat').cast(pl.Float32, strict=False),\n",
    "        pl.col('end_lng').cast(pl.Float32, strict=False),\n",
    "        pl.col('member_casual').cast(pl.Utf8, strict=False),\n",
    "    )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date_range in date_ranges_2:\n",
    "    filename = date_range[0][:4] + '.parquet'\n",
    "    df = create_df(date_range, datapath)\n",
    "    df = cast_types_2(df)\n",
    "\n",
    "    df.write_parquet(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decisions made while processing the data\n",
    "\n",
    "After processing the data, I decided against joining all the data for two reasons:\n",
    "1. The data would be too large to process on my machine. \n",
    "    - Even with the reduced file size (4.93GB vs 36.79GB), a single dataframe would be cumbersome to work with.\n",
    "2. The data is not consistent across the two date ranges.\n",
    "    - The station ID is different\n",
    "    - bike_id is not available for the second date range\n",
    "    - rideable_type and ride_id is not available for the first date range\n",
    " \n",
    " I did not want to drop bike_id and rideable_type because they are interesting datapoints to explore. \n",
    "\n",
    "#### bike_id\n",
    "Looking at the data from 2013 - 2021, I noticed that the bike_id might be a interesting datapoint to look at. You could potentially trace the history of a bike and see how many times it was used, how long it was used for, and where it was used. From 2021 to 2023, bike_id is replaced by ride_id. We are not able to create the same analysis for ride_id because it is a unique identified for each ride.\n",
    "\n",
    "#### rideable_type\n",
    "Rideable_type was added in 2021, with the values being: \"classic_bike\", \"electric_bike\" and \"docked_bike\". Although E-bikes were added to the Citi Bike fleet in 2018, the datapoint is not available until 2021. \n",
    " \n",
    "\n",
    "We have three options:\n",
    "1. Drop the column\n",
    "2. Fill in the data (e.g. assume that all rides from 2013 - 2018 were on classic bikes, and all rides from 2018 - 2021 as mix)\n",
    "3. Create a model to predict the rideable_type based on the other data points\n",
    "4. Keep the data as-is\n",
    " \n",
    " \n",
    " We'll go with option 4 for now, and revisit option 3 in the future.\n",
    "\n",
    "#### Saving the data\n",
    "The data is saved as a parquet file, one for each year. \n",
    " \n",
    "Saving as a parquet file significantly reduces the file size and allows for faster processing. By having a separate parquet file for each year, we can keep a consistent approach in analyzing the data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-cb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
